Here’s a **complete example** of pricing an **American-style put option** using both:  
1. **PDE with Finite Difference Method (FDM)** (handling early exercise via penalty method).  
2. **Monte Carlo with Longstaff-Schwartz regression**.  

We’ll compare results and discuss pros/cons.  

---

### **Problem Setup**  
- **Option**: American Put (strike \(K\), maturity \(T\)).  
- **Early Exercise**: Can exercise at any time \(\tau \leq T\) for payoff \(\max(K - S_\tau, 0)\).  
- **Model**: Geometric Brownian Motion (GBM):  
  \[
  dS_t = r S_t dt + \sigma S_t dW_t.
  \]  

---

### **1. PDE Approach (FDM + Penalty Method)**  
**PDE with Early Exercise Constraint**:  
\[
\frac{\partial V}{\partial t} + \frac{1}{2} \sigma^2 S^2 \frac{\partial^2 V}{\partial S^2} + r S \frac{\partial V}{\partial S} - r V \leq 0,
\]  
with **free boundary condition**:  
\[
V(S, t) \geq \max(K - S, 0).
\]  

**Numerical Solution**:  
- Discretize using **Crank-Nicolson**.  
- **Penalty Method**: Force \(V \geq \text{payoff}\) at each timestep.  

#### **Python Code (PDE-FDM)**  
```python
import numpy as np
from scipy.sparse import diags
from scipy.sparse.linalg import spsolve

def american_put_pde(S0, K, T, r, sigma, N=1000, M=100):
    dt = T / M
    S_max = 3 * K
    dS = S_max / N
    S = np.linspace(0, S_max, N+1)
    V = np.maximum(K - S, 0)
    
    for t in range(M):
        # Crank-Nicolson coefficients
        alpha = 0.25 * dt * (sigma**2 * S**2 / dS**2 - r * S / dS)
        beta = -0.5 * dt * (sigma**2 * S**2 / dS**2 + r)
        A = diags([-alpha[1:], 1 - beta, -alpha[:-1]], [-1, 0, 1])
        B = diags([alpha[1:], 1 + beta, alpha[:-1]], [-1, 0, 1])
        
        # Solve and enforce early exercise
        V = np.maximum(spsolve(A, B.dot(V)), np.maximum(K - S, 0))
    
    return np.interp(S0, S, V)

# Example
S0 = 100; K = 100; T = 1; r = 0.05; sigma = 0.3
pde_price = american_put_pde(S0, K, T, r, sigma)
print(f"PDE Price (American Put): {pde_price:.4f}")
```

---

### **2. Monte Carlo + Longstaff-Schwartz**  
**Key Idea**:  
- Simulate paths forward.  
- At each step, use regression to estimate the **continuation value**.  
- Exercise if immediate payoff > continuation value.  

#### **Python Code (Monte Carlo)**  
```python
def american_put_mc(S0, K, T, r, sigma, N=10000, M=100):
    dt = T / M
    S = np.zeros((N, M+1))
    S[:, 0] = S0
    
    # Simulate paths
    for t in range(1, M+1):
        Z = np.random.normal(0, 1, N)
        S[:, t] = S[:, t-1] * np.exp((r - 0.5*sigma**2)*dt + sigma*np.sqrt(dt)*Z)
    
    # Backward induction
    payoff = np.maximum(K - S[:, -1], 0)
    
    for t in range(M-1, 0, -1):
        in_the_money = S[:, t] < K
        X = S[in_the_money, t].reshape(-1, 1)
        Y = payoff[in_the_money] * np.exp(-r*dt)
        
        # Linear regression (could use polynomial features)
        from sklearn.linear_model import LinearRegression
        reg = LinearRegression().fit(X, Y)
        continuation = reg.predict(X)
        
        exercise = np.maximum(K - X[:, 0], 0)
        payoff[in_the_money] = np.where(exercise > continuation, exercise, Y)
    
    return np.exp(-r*dt) * np.mean(payoff)

mc_price = american_put_mc(S0, K, T, r, sigma)
print(f"MC Price (American Put): {mc_price:.4f}")
```

---

### **Results Comparison**  
| Method               | Price (S=100, K=100) | Runtime | Pros/Cons |
|----------------------|----------------------|---------|-----------|
| **PDE (FDM)**        | ~6.80                | Fast (ms) | High accuracy for 1D, but harder for high dimensions. |
| **MC (Longstaff-Schwartz)** | ~6.75        | Slow (seconds) | Handles path-dependency, scales to high dimensions. |

---

### **Key Takeaways**  
1. **PDE (FDM)**:  
   - Best for **low-dimensional** problems (1D-2D).  
   - Requires careful handling of early exercise (penalty/PSOR).  
2. **Monte Carlo + Longstaff-Schwartz**:  
   - Handles **high dimensions** (e.g., basket options).  
   - Regression step is computationally expensive.  

---

### **Extensions**  
1. **Stochastic Volatility**: Modify SDE to Heston (MC handles this easily; PDE becomes 2D).  
2. **Higher-Order Basis Functions**: Replace linear regression with polynomials for better continuation estimates.  
3. **GPU Acceleration**: Speed up Monte Carlo using `numba` or CUDA.  

Would you like the full code with convergence plots?